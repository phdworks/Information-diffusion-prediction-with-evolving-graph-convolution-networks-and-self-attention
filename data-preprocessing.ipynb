{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-30T19:40:59.506568Z",
     "iopub.status.busy": "2021-01-30T19:40:59.501449Z",
     "iopub.status.idle": "2021-01-30T19:42:10.023500Z",
     "shell.execute_reply": "2021-01-30T19:42:10.022670Z"
    },
    "papermill": {
     "duration": 70.546324,
     "end_time": "2021-01-30T19:42:10.023698",
     "exception": false,
     "start_time": "2021-01-30T19:40:59.477374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\r\n",
      "10.2\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n",
    "!pip -q install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n",
    "!pip -q install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n",
    "!pip -q install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n",
    "!pip -q install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html\n",
    "!pip -q install torch-geometric\n",
    "!pip -q install torch-geometric-temporal\n",
    "\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import os \n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import math\n",
    "import time \n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Namespace(object):\n",
    "    '''\n",
    "    helps referencing object in a dictionary as dict.key instead of dict['key']\n",
    "    '''\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-30T19:42:10.056367Z",
     "iopub.status.busy": "2021-01-30T19:42:10.045945Z",
     "iopub.status.idle": "2021-01-30T19:42:20.305725Z",
     "shell.execute_reply": "2021-01-30T19:42:20.304501Z"
    },
    "papermill": {
     "duration": 10.274685,
     "end_time": "2021-01-30T19:42:20.305878",
     "exception": false,
     "start_time": "2021-01-30T19:42:10.031193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoint/\n",
    "\n",
    "Constants = Namespace({})\n",
    "Constants.PAD = 0\n",
    "Constants.UNK = 2\n",
    "Constants.BOS = 3\n",
    "Constants.EOS = 1\n",
    "\n",
    "Constants.time_step_split = 8\n",
    "Constants.n_heads = 14\n",
    "Constants.step_len = 5 \n",
    "\n",
    "class Options(object):    \n",
    "    def __init__(self, data_name = 'twitter'):\n",
    "        #data options.\n",
    "        #data_name = 'twitter'\n",
    "        #train file path.\n",
    "        self.train_data = data_name+'/cascade.txt'\n",
    "        #valid file path.\n",
    "        self.valid_data = data_name+'/cascadevalid.txt'\n",
    "        #test file path.\n",
    "        self.test_data = data_name+'/cascadetest.txt'\n",
    "        self.u2idx_dict = data_name+'/u2idx.pickle'\n",
    "        self.idx2u_dict = data_name+'/idx2u.pickle'\n",
    "        #save path.\n",
    "        self.save_path = ''\n",
    "        self.batch_size = 32\n",
    "        self.net_data = data_name+'/edges.txt'\n",
    "        self.embed_dim = 64\n",
    "        # self.embed_file = data_name+'/dw'+str(self.embed_dim)+'.txt'\n",
    "        \n",
    "        \n",
    "root_path = './' \n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "\n",
    "opt = Namespace({})\n",
    "opt.epoch=50\n",
    "opt.batch_size=16\n",
    "opt.d_model=64\n",
    "opt.n_warmup_steps=1000\n",
    "opt.dropout=0.1\n",
    "opt.log=None\n",
    "opt.save_path=root_path + \"checkpoint/DiffusionPrediction.pt\"\n",
    "opt.save_mode='best'\n",
    "opt.network=False # use social network; need features or deepwalk embeddings as initial input\n",
    "opt.pos_emb=True\n",
    "opt.warmup=10 # warmup epochs\n",
    "opt.notes=''\n",
    "opt.d_word_vec = opt.d_model\n",
    "\n",
    "\n",
    "data_path = \"../input/dyhgcn-data/data/twitter\"\n",
    "\n",
    "options = Options(data_path)\n",
    "_u2idx = {}\n",
    "_idx2u = []\n",
    "\n",
    "\n",
    "with open(options.u2idx_dict, 'rb') as handle:\n",
    "    _u2idx = pickle.load(handle)\n",
    "with open(options.idx2u_dict, 'rb') as handle:\n",
    "    _idx2u = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "follow_relation = []  # directed relation\n",
    "if os.path.exists(options.net_data):\n",
    "    with open(options.net_data, 'r') as handle:\n",
    "        edges_list = handle.read().strip().split(\"\\n\")\n",
    "        edges_list = [edge.split(',') for edge in edges_list]\n",
    "        follow_relation = [(_u2idx[edge[0]], _u2idx[edge[1]]) for edge in edges_list if edge[0] in _u2idx and edge[1] in _u2idx]\n",
    "\n",
    "        \n",
    "if not os.path.exists(data_path + \"/diffusion_graph.csv\"):\n",
    "    options = Options(data_path)\n",
    "\n",
    "    with open(options.train_data, 'r') as handle:\n",
    "        cascade_list = handle.read().strip().split(\"\\n\")\n",
    "        cascade_list = [chunk.strip().split(\" \") for chunk in cascade_list if len(chunk.strip()) != 0]\n",
    "\n",
    "\n",
    "    t_cascades = []\n",
    "    for chunks in cascade_list:\n",
    "        userlist = [chunk.split(',') for chunk in chunks]\n",
    "        userlist = [[_u2idx[x[0]], int(x[1])] for x in userlist if x[0] in _u2idx]\n",
    "        pair_user = [(i[0], j[0], j[1]) for i, j in zip(userlist[::1], userlist[1::1])]\n",
    "        if len(pair_user) > 1 and len(pair_user) <= 500:\n",
    "            t_cascades += pair_user\n",
    "\n",
    "    t_cascades_pd = pd.DataFrame(t_cascades)\n",
    "    t_cascades_pd.columns = [\"user1\", \"user2\", \"timestamp\"]\n",
    "    t_cascades_pd.to_csv( \"diffusion_graph.csv\", index=False)\n",
    "else:\n",
    "    t_cascades_pd = pd.read_csv(data_path + \"/diffusion_graph.csv\")\n",
    "\n",
    "t_cascades_pd = t_cascades_pd.sort_values(by=\"timestamp\")\n",
    "\n",
    "t_cascades_length = t_cascades_pd.shape[0]\n",
    "step_length_x = t_cascades_length // Constants.time_step_split\n",
    "\n",
    "\n",
    "t_cascades_list = dict() \n",
    "for x in range(step_length_x, t_cascades_length-step_length_x, step_length_x):\n",
    "    #break;\n",
    "    t_cascades_pd_sub = t_cascades_pd[:x]\n",
    "    t_cascades_sub_list = t_cascades_pd_sub.apply(lambda x: (x[\"user1\"], x[\"user2\"]), axis=1).tolist()\n",
    "    sub_timesas = t_cascades_pd_sub[\"timestamp\"].max()\n",
    "    # t_cascades_list.append(t_cascades_sub_list)\n",
    "    t_cascades_list[sub_timesas] = t_cascades_sub_list\n",
    "    \n",
    "# + all\n",
    "t_cascades_sub_list = t_cascades_pd.apply(lambda x: (x[\"user1\"], x[\"user2\"]), axis=1).tolist()\n",
    "# t_cascades_list.append(t_cascades_sub_list) \n",
    "sub_timesas = t_cascades_pd[\"timestamp\"].max()\n",
    "t_cascades_list[sub_timesas] = t_cascades_sub_list\n",
    "\n",
    "dynamic_graph_dict_list = dict() \n",
    "for key in sorted(t_cascades_list.keys()): \n",
    "    edges_list = t_cascades_list[key]\n",
    "    # edges_list_tensor = torch.LongTensor(edges_list).t() \n",
    "    # loader = DataLoader(dataset, batch_size=32, shuffle=True) \n",
    "    # data = Data(edge_index=edges_list_tensor)\n",
    "    cascade_dic = defaultdict(list)\n",
    "    for upair in edges_list:\n",
    "        cascade_dic[upair].append(1) \n",
    "    dynamic_graph_dict_list[key] = cascade_dic\n",
    "    \n",
    "\n",
    "dynamic_graph = dict()\n",
    "for x in sorted(dynamic_graph_dict_list.keys()):\n",
    "    edges_list = follow_relation\n",
    "    edges_type_list = [0] * len(follow_relation)  # 0:follow relation,  1:repost relation\n",
    "    edges_weight = [1.0] * len(follow_relation)\n",
    "    for key, value in dynamic_graph_dict_list[x].items():\n",
    "        \n",
    "        edges_list.append(key)\n",
    "        edges_type_list.append(1)\n",
    "        edges_weight.append(sum(value))\n",
    "\n",
    "    edges_list_tensor = torch.LongTensor(edges_list).t()\n",
    "    edges_type = torch.LongTensor(edges_type_list)\n",
    "    edges_weight = torch.FloatTensor(edges_weight)\n",
    "\n",
    "    data = Data(edge_index=edges_list_tensor, edge_type=edges_type, edge_weight=edges_weight)\n",
    "    dynamic_graph[x] = data \n",
    "    \n",
    "diffusion_graph = dynamic_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-30T19:42:20.355959Z",
     "iopub.status.busy": "2021-01-30T19:42:20.340734Z",
     "iopub.status.idle": "2021-01-30T19:42:20.358526Z",
     "shell.execute_reply": "2021-01-30T19:42:20.358128Z"
    },
    "papermill": {
     "duration": 0.044861,
     "end_time": "2021-01-30T19:42:20.358633",
     "exception": false,
     "start_time": "2021-01-30T19:42:20.313772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataConstruct(object):\n",
    "    ''' For data iteration '''\n",
    "\n",
    "    def __init__(\n",
    "            self, data_name, data=0, load_dict=True, cuda=True, batch_size=32, shuffle=True, test=False, with_EOS=True): #data = 0 for train, 1 for valid, 2 for test\n",
    "        self.options = Options(data_name)\n",
    "        self.options.batch_size = batch_size\n",
    "        self._u2idx = {}\n",
    "        self._idx2u = []\n",
    "        self.data = data\n",
    "        self.test = test\n",
    "        self.with_EOS = with_EOS\n",
    "        if not load_dict:\n",
    "            raise NotImplementedError\n",
    "            #self._buildIndex()\n",
    "\n",
    "        else:\n",
    "            with open(self.options.u2idx_dict, 'rb') as handle:\n",
    "                self._u2idx = pickle.load(handle)\n",
    "            with open(self.options.idx2u_dict, 'rb') as handle:\n",
    "                self._idx2u = pickle.load(handle)\n",
    "            self.user_size = len(self._u2idx)\n",
    "        self._train_cascades,train_len = self._readFromFile(self.options.train_data)\n",
    "        self._valid_cascades,valid_len = self._readFromFile(self.options.valid_data)\n",
    "        self._test_cascades,test_len = self._readFromFile(self.options.test_data)\n",
    "        self._train_cascades_timestamp = self._readFromFileTimestamp(self.options.train_data)\n",
    "        self._valid_cascades_timestamp = self._readFromFileTimestamp(self.options.valid_data)\n",
    "        self._test_cascades_timestamp = self._readFromFileTimestamp(self.options.test_data)\n",
    "\n",
    "        self.train_size = len(self._train_cascades)\n",
    "        self.valid_size = len(self._valid_cascades)\n",
    "        self.test_size = len(self._test_cascades)\n",
    "        self.cuda = cuda\n",
    "\n",
    "        if self.data == 0:\n",
    "            self._n_batch = int(np.ceil(len(self._train_cascades) / batch_size))\n",
    "        elif self.data == 1:\n",
    "            self._n_batch = int(np.ceil(len(self._valid_cascades) / batch_size))\n",
    "        else:\n",
    "            self._n_batch = int(np.ceil(len(self._test_cascades) / batch_size))\n",
    "\n",
    "        self._batch_size = self.options.batch_size\n",
    "\n",
    "        self._iter_count = 0\n",
    "\n",
    "        self._need_shuffle = shuffle\n",
    "\n",
    "        if self._need_shuffle:\n",
    "            random_seed_int = random.randint(0, 1000)\n",
    "            random.seed(random_seed_int)\n",
    "            random.shuffle(self._train_cascades)\n",
    "            random.seed(random_seed_int)\n",
    "            random.shuffle(self._train_cascades_timestamp)\n",
    "\n",
    "    def _readFromFile(self, filename):\n",
    "        \"\"\"read all cascade from training or testing files. \"\"\"\n",
    "        total_len = 0\n",
    "        t_cascades = []\n",
    "        for line in open(filename):\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            userlist = []\n",
    "            chunks = line.strip().split()\n",
    "            for chunk in chunks:\n",
    "                #try:\n",
    "                user, timestamp = chunk.split(',')\n",
    "                # except:\n",
    "                #     print(chunk)\n",
    "                if user in self._u2idx:\n",
    "                    userlist.append(self._u2idx[user])\n",
    "\n",
    "            if len(userlist) > 1 and len(userlist)<=500:\n",
    "                total_len+=len(userlist)\n",
    "                if self.with_EOS:\n",
    "                    userlist.append(Constants.EOS)\n",
    "                t_cascades.append(userlist)\n",
    "        return t_cascades,total_len\n",
    "\n",
    "    def _readFromFileTimestamp(self, filename):\n",
    "        \"\"\"read all cascade from training or testing files. \"\"\"\n",
    "        t_cascades = []\n",
    "        for line in open(filename):\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            timestamplist = []\n",
    "            chunks = line.strip().split()\n",
    "            for chunk in chunks:\n",
    "                # try:\n",
    "                user, timestamp = chunk.split(',')\n",
    "                timestamp = int(timestamp)\n",
    "                # timestamp = timestamp // (60 * 60 * 24)\n",
    "                # except:\n",
    "                #     print(chunk)\n",
    "                if user in self._u2idx:\n",
    "                    timestamplist.append(timestamp)\n",
    "\n",
    "            if len(timestamplist) > 1 and len(timestamplist)<=500:\n",
    "                if self.with_EOS:\n",
    "                    timestamplist.append(Constants.EOS)\n",
    "                t_cascades.append(timestamplist)\n",
    "        return t_cascades\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_batch\n",
    "\n",
    "    def next(self):\n",
    "        ''' Get the next batch '''\n",
    "        def pad_to_longest(insts):\n",
    "            ''' Pad the instance to the max seq length in batch '''\n",
    "\n",
    "            max_len = max(len(inst) for inst in insts)\n",
    "\n",
    "            inst_data = np.array([\n",
    "                inst + [Constants.PAD] * (max_len - len(inst))\n",
    "                for inst in insts])\n",
    "        \n",
    "            inst_data_tensor = Variable(\n",
    "                torch.LongTensor(inst_data), volatile=self.test)\n",
    "\n",
    "            if self.cuda:\n",
    "                inst_data_tensor = inst_data_tensor.cuda()\n",
    "\n",
    "            return inst_data_tensor\n",
    "\n",
    "        if self._iter_count < self._n_batch:\n",
    "            batch_idx = self._iter_count\n",
    "            self._iter_count += 1\n",
    "\n",
    "            start_idx = batch_idx * self._batch_size\n",
    "            end_idx = (batch_idx + 1) * self._batch_size\n",
    "\n",
    "            if self.data == 0:\n",
    "                seq_insts = self._train_cascades[start_idx:end_idx]\n",
    "                seq_timestamp = self._train_cascades_timestamp[start_idx:end_idx]\n",
    "            elif self.data == 1:\n",
    "                seq_insts = self._valid_cascades[start_idx:end_idx]\n",
    "                seq_timestamp = self._valid_cascades[start_idx:end_idx]\n",
    "            else:\n",
    "                seq_insts = self._test_cascades[start_idx:end_idx]\n",
    "                seq_timestamp = self._test_cascades_timestamp[start_idx:end_idx]\n",
    "                \n",
    "            seq_data = pad_to_longest(seq_insts)\n",
    "            seq_data_timestamp = pad_to_longest(seq_timestamp)\n",
    "            \n",
    "            return seq_data, seq_data_timestamp\n",
    "        else:\n",
    "\n",
    "            if self._need_shuffle:\n",
    "                random.shuffle(self._train_cascades)\n",
    "                #random.shuffle(self._test_cascades)\n",
    "\n",
    "            self._iter_count = 0\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-30T19:42:20.377712Z",
     "iopub.status.busy": "2021-01-30T19:42:20.377222Z",
     "iopub.status.idle": "2021-01-30T19:42:21.355381Z",
     "shell.execute_reply": "2021-01-30T19:42:21.354706Z"
    },
    "papermill": {
     "duration": 0.989716,
     "end_time": "2021-01-30T19:42:21.355569",
     "exception": false,
     "start_time": "2021-01-30T19:42:20.365853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = DataConstruct(data_path, data=0, load_dict=True, batch_size=opt.batch_size, cuda=torch.cuda.is_available())\n",
    "valid_data = DataConstruct(data_path, data=1, batch_size=opt.batch_size, cuda=torch.cuda.is_available()) # torch.cuda.is_available()\n",
    "test_data = DataConstruct(data_path, data=2, batch_size=opt.batch_size, cuda=torch.cuda.is_available())\n",
    "\n",
    "opt.user_size = train_data.user_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-30T19:42:21.387760Z",
     "iopub.status.busy": "2021-01-30T19:42:21.386943Z",
     "iopub.status.idle": "2021-01-30T19:42:21.389094Z",
     "shell.execute_reply": "2021-01-30T19:42:21.388453Z"
    },
    "papermill": {
     "duration": 0.020935,
     "end_time": "2021-01-30T19:42:21.389239",
     "exception": false,
     "start_time": "2021-01-30T19:42:21.368304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-30T19:42:21.421181Z",
     "iopub.status.busy": "2021-01-30T19:42:21.420441Z",
     "iopub.status.idle": "2021-01-30T19:42:24.078317Z",
     "shell.execute_reply": "2021-01-30T19:42:24.077652Z"
    },
    "papermill": {
     "duration": 2.67724,
     "end_time": "2021-01-30T19:42:24.078461",
     "exception": false,
     "start_time": "2021-01-30T19:42:21.401221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data.pkl', 'wb') as f:\n",
    "    dill.dump(train_data, f)\n",
    "    dill.dump(valid_data, f)\n",
    "    dill.dump(test_data, f)\n",
    "    dill.dump(diffusion_graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007352,
     "end_time": "2021-01-30T19:42:24.093560",
     "exception": false,
     "start_time": "2021-01-30T19:42:24.086208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 91.145644,
   "end_time": "2021-01-30T19:42:26.014580",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-30T19:40:54.868936",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
